---
title: "P8106_Midterm"
author: "jck2183_Chia-wen Kao"
date: "2021/3/26"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = T, message = FALSE, results='hide', warning=FALSE}
library(tidyverse)
library(caret)
library(glmnet)
library(mlbench)
library(pROC) #generate ROC curve and calculate AUC 
library(pdp) #partial dependent plot
library(vip) #variable importance plot: global impact on different predictor
library(AppliedPredictiveModeling) # for visualization purpose
library(corrplot)
library(RColorBrewer)
library(RANN)
library(visdat)
library(mgcv)
```

## Introduction: 

According to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths. This dataset is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. Each row in the data provides relavant information about the patient.

Data Source: https://www.kaggle.com/fedesoriano/stroke-prediction-dataset

All the features we had:

- id: unique identifier
- gender: "Male", "Female" or "Other"
- age: age of the patient
- hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension
- heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease
- ever_married: "No" or "Yes"
- work_type: "children", "Govt_jov", "Never_worked", "Private" or "Self-employed"
- Residence_type: "Rural" or "Urban"
- avg_glucose_level: average glucose level in blood
- bmi: body mass index
- smoking_status: "formerly smoked", "never smoked", "smokes" or "Unknown"*
- stroke: 1 if the patient had a stroke or 0 if not
*Note: "Unknown" in smoking_status means that the information is unavailable for this patient


### Import Data
```{r data prep}
stroke_df = read.csv("./data/healthcare-dataset-stroke-data.csv")
# head(stroke_df)

stroke_df$stroke = as.factor(stroke_df$stroke)
stroke_df$gender = factor(stroke_df$gender) %>% as.numeric()
stroke_df$ever_married = factor(stroke_df$ever_married) %>% as.numeric()
stroke_df$work_type = factor(stroke_df$work_type) %>% as.numeric()
stroke_df$Residence_type = factor(stroke_df$Residence_type) %>% as.numeric()
stroke_df$smoking_status = factor(stroke_df$smoking_status) %>% as.numeric()
stroke_df$heart_disease = factor(stroke_df$heart_disease) %>% as.numeric()
stroke_df$hypertension = as.numeric(factor(stroke_df$hypertension))
stroke_df$work_type = as.factor(stroke_df$work_type) %>% as.numeric()
stroke_df$bmi = as.numeric(stroke_df$bmi)

stroke_df = stroke_df[, -1] %>% 
    mutate(stroke = recode(stroke, 
                           `0` = "No", 
                           `1` = "Yes"), 
           stroke = factor(stroke)) %>% 
    filter(gender < 3) 


summary(stroke_df)
vis_miss(stroke_df)

```

The imported dataset has 5110 observations in total. Excluding the id, we only gave ten features and one binary outcome variable-stroke (0:no stroke, 1:stroke). We found that the stroke outcome distribution is imbalanced with 4861 observations have no stroke while 249 observations have a stroke.

We find out there are 201 observations with missing values in BMI. Among these missing values, 40 observations have a stroke while 161 observations without stroke. We will then apply preprocess imputation in the caret train function to address the imputation problem. We also have 1544 unknown in smoke status, will treat those who answered unknown as a variable so no need to impute them.

Our main task is to find out the appropriate models that have a better performance on prediction by comparing several models' performance. 

First, we have to convert character variables into factors to add them into our model and proceed with the analysis. Plus, we will also examine if there is any correlation among features. Meanwhile, we also found there is an observation who identified their gender as "Other". We decide to omit this single subject so that we can proceed with our analysis.


Next, the characteristics of features will help us determine which model would be proper. As the outcome is binary, and the features are mixtures of continuous and categorical variables. We also have to decide how to partition the train and test data, which cross-validation method to use. Evaluation metrics should be used and set up a reasonable tuning grid corresponding to the tuning parameter.

## Exploratory Data Analysis
Partition the dataset, I will use 70% as training data and 30% as test data.
```{r}
set.seed(123)
trRow = createDataPartition(y = stroke_df$stroke, p = 0.7, list = F)
train.data = stroke_df[trRow, ]
test.data = stroke_df[-trRow, ] 
```
Try imputation with `preProcess() `
```{r}
knnImp = preProcess(train.data, method = "knnImpute", k = 3)
train.data = predict(knnImp, train.data)
vis_miss(train.data)
test.data = predict(knnImp,test.data)
vis_miss(test.data)
```
Try following models to see which algorithm fits the best because our outcome is binary and it would better to proceed with which classification performs the best. We will have accuracy and ROC/AUC as our evaluation metrics.

### Logistic Regression
```{r}
set.seed(123)
ctrl = trainControl(method = "cv", 
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

ctrl1 = trainControl(method = "cv", 
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE, sampling = 'smote')

lm.fit = train( x = train.data[, c(1:10)],
                   y = train.data$stroke,
                   method = "glm",
                   metric = "ROC",
                   trControl = ctrl)

lm.fit1 = train( x = train.data[, c(1:10)],
                   y = train.data$stroke,
                   method = "glm",
                   metric = "ROC",
                   trControl = ctrl1)

lm.pred = predict(lm.fit, newdata = test.data,
                          type = "prob")

lm.prob = ifelse(lm.pred$Yes > 0.5, "Yes", "No")



confusionMatrix(data = as.factor(lm.prob),
                reference = test.data$stroke,
                positive = "Yes")
roc.lm = roc(test.data$stroke, lm.pred[,2])

auc.lm = roc.lm$auc[1]
auc.lm
plot(roc.lm, legacy.axes = TRUE, print.auc = TRUE) 
plot(smooth(roc.lm), col = 4, add = TRUE)
```

### Penalized logistic regression
To add penalty to our loss, we can shrink the coefficients of correlated predictors towards each other by tuning alpha and lambda.
```{r}
set.seed(123)
glmnGrid = expand.grid(.alpha = seq(0, 1, length = 6),
                       .lambda = exp(seq(-8, -2, length = 20)))

model.glmn = train( x = train.data[,c(1:10)],
                    y = train.data$stroke,
                    method = "glmnet",
                    tuneGrid = glmnGrid,
                    metric = "ROC",
                    trControl = ctrl)

model.glmn1 = train( x = train.data[,c(1:10)],
                    y = train.data$stroke,
                    method = "glmnet",
                    tuneGrid = glmnGrid,
                    metric = "ROC",
                    trControl = ctrl1)

plot(model.glmn, xTrans = function(x) log(x))   

model.glmn$bestTune

glmn.pred = predict(model.glmn, newdata = test.data, type = "prob")
glmn.prob = ifelse(glmn.pred$Yes > 0.5, "Yes", "No")
confusionMatrix(data = as.factor(glmn.prob),
                reference = test.data$stroke,
                positive = "Yes")

roc.glmn = roc(test.data$stroke, glmn.pred[,2])

auc.glmn = roc.glmn$auc[1]
auc.glmn
plot(roc.glmn, legacy.axes = TRUE, print.auc = TRUE) 
plot(smooth(roc.glmn), col = 4, add = TRUE)
```


### Ridge Regression
Ridge can also help us shrink the coefficients of correlated predictors towards each other by tuning only lambda.
```{r}
set.seed(123)
ridge.fit = train( x = train.data[,c(1:10)], 
                   y = train.data$stroke,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 0, #ridge
                                          lambda = exp(seq(10, -2, length=100))),
                   preProc = c("center", "scale"),
                   trControl = ctrl)

ridge.fit1 = train( x = train.data[,c(1:10)], 
                   y = train.data$stroke,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 0, #ridge
                                          lambda = exp(seq(10, -2, length=100))),
                   preProc = c("center", "scale"),
                   trControl = ctrl1)
#need to specify 2 tunning parameters.
plot(ridge.fit, xTrans = log)

ridge.pred = predict(ridge.fit, newdata = test.data, type = "prob")
ridge.prob = ifelse(ridge.pred$Yes > 0.5, "Yes", "No")
confusionMatrix(data = as.factor(ridge.prob),
                reference = test.data$stroke,
                positive = "Yes")
roc.ridge = roc(test.data$stroke, ridge.pred[,2])

auc.ridge = roc.ridge$auc[1]
auc.ridge
plot(roc.ridge, legacy.axes = TRUE, print.auc = TRUE) 
plot(smooth(roc.ridge), col = 4, add = TRUE)
```
### LDA
If we want to use LDA we have to make the assumption that the predictors have Gaussian-alike distribution.
```{r}
set.seed(123)
lda.fit = train(   x = train.data[,c(1:10)],
                   y = train.data$stroke,
                   method = "lda",
                   metric = "ROC",
                   trControl = ctrl)

lda.fit1 = train(   x = train.data[,c(1:10)],
                   y = train.data$stroke,
                   method = "lda",
                   metric = "ROC",
                   trControl = ctrl1)

lda.pred = 
    predict(lda.fit, newdata = test.data, type = "prob")

lda.pred

lda.prob = ifelse(lda.pred$Yes > 0.5, "Yes", "No")

confusionMatrix(data = as.factor(lda.prob),
                reference = test.data$stroke,
                positive = "Yes")

roc.lda = roc(test.data$stroke, lda.pred[,2])

auc.lda = roc.lda$auc[1]
auc.lda
plot(roc.lda, legacy.axes = TRUE, print.auc = TRUE) 
plot(smooth(roc.lda), col = 4, add = TRUE)

varImp(lda.fit)
```

### KNN
```{r}
set.seed(123)
knn.fit = train(   x = train.data[, c(1:10)],
                   y = train.data$stroke,
                   method = "knn",
                   preProcess = c("center","scale"),
                   tuneGrid = data.frame(k = seq(1,200,by=5)),
                   trControl = ctrl)

knn.fit1 = train(  x = train.data[, c(1:10)],
                   y = train.data$stroke,
                   method = "knn",
                   preProcess = c("center","scale"),
                   tuneGrid = data.frame(k = seq(1,200,by=5)),
                   trControl = ctrl1)

knn.fit$finalModel

knn.pred = 
    predict(knn.fit, newdata = test.data, type = "prob")

knn.prob = ifelse(knn.pred$Yes > 0.5, "Yes", "No")

confusionMatrix(data = as.factor(knn.prob),
                reference = test.data$stroke,
                positive = "Yes")

roc.knn = roc(test.data$stroke, knn.pred[,2])

auc.knn = roc.knn$auc[1]
auc.knn
plot(roc.knn, legacy.axes = TRUE, print.auc = TRUE) 
plot(smooth(roc.knn), col = 4, add = TRUE)
```
Evaluation the ROC by resampling these models(with normal sampling method).
```{r normal sampling}
res = resamples(list(GLM = lm.fit, 
                      GLMNET = model.glmn, 
                      RIDGE = ridge.fit,
                      LDA = lda.fit,
                      KNN = knn.fit))
summary(res)

bwplot(res, metric = "ROC")
```
Evaluation the ROC by resampling these models(with smote sampling method).
```{r sample with smote}
res1 = resamples(list(GLM = lm.fit1, 
                      GLMNET = model.glmn1, 
                      RIDGE = ridge.fit1,
                      LDA = lda.fit1,
                      KNN = knn.fit1))
summary(res1)

bwplot(res1, metric = "ROC")
```

## Variable Importance

```{r}
varImp(model.glmn)
```
We can see that from the best performance model: Penalized Regression Model, Age is the most important variable in determining having outcome of interest.


## Conclusion and Model limitation

Based on the Exploratory Data Analysis, we can see that only 5% of all the people in the dataset had a stroke at some point. This means that our baseline dummy model has an accuracy of 95%. I tried to use smote sampling to adjust the unbalanced dataset and found that the Penalized Logistic Regression outperformed Ridge Regression, LDA, Logistic Regression, and KNN. However, when I removed the smote sampling from the train control function, the rank fluctuated. Penalized Logistic Regression still performed the best, while logistic regression came the next, with LDA, Ridge Regression, and KNN followed. Ridge Regression performed the best between the two sampling methods, and KNN performed the worst. It is because Logistic regression is popular for classification when K= 2(stroke vs. non-stroke). In contrast, Ridge regression is to shrink the coefficients of correlated predictors towards each other. On the other hand, LDA is more appropriate when the sample size is small, or the classes are well separated, and Gaussian assumptions are reasonable, as well as when the class greater than two categories. KNN is easier to be affected by outliers.

I will use the evaluation based on the normal sampling way to avoid the biased prediction results because of oversampling diseased reality distribution/prevalence.